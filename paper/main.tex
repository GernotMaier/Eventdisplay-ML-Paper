\documentclass[12pt]{article}
\usepackage{aas_macros}
\usepackage{natbib}
\usepackage[utf8]{inputenc}
\usepackage[left=3cm,right=3cm,top=3cm,bottom=3cm]{geometry}
\usepackage{color}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue]{hyperref}
 \usepackage{graphicx}
 \usepackage{subfigure}
 \usepackage{pythonhighlight}
 \usepackage{comment}
 \usepackage{tablefootnote}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{booktabs} % toprule, etc.
\usepackage{makecell}
\usepackage{soul}
\usepackage{dirtree}
\sethlcolor{lightgray}
\usepackage{listings}
\lstset{basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{red},
  keywordstyle=\color{blue},
  backgroundcolor=\color{lightgray},
  numbers=left
}
\lstdefinestyle{yaml}{
     basicstyle=\color{blue}\footnotesize,
     rulecolor=\color{black},
     string=[s]{'}{'},
     stringstyle=\color{blue},
     comment=[l]{:},
     commentstyle=\color{black},
     morecomment=[l]{-}
}
\usepackage[most]{tcolorbox}

%textmarker style from colorbox doc
\tcbset{textmarker/.style={%
        enhanced,
        parbox=false,boxrule=0mm,boxsep=0mm,arc=0mm,
        outer arc=0mm,left=6mm,right=3mm,top=7pt,bottom=7pt,
        toptitle=1mm,bottomtitle=1mm,oversize}}


% define new colorboxes
\newtcolorbox{hintBox}{textmarker,
    borderline west={6pt}{0pt}{yellow},
    colback=yellow!10!white}
\newtcolorbox{importantBox}{textmarker,
    borderline west={6pt}{0pt}{red},
    colback=red!10!white}
\newtcolorbox{noteBox}{textmarker,
    borderline west={6pt}{0pt}{green},
    colback=green!10!white}

% define commands for easy access
\newcommand{\note}[1]{\begin{noteBox} \textbf{Note:} #1 \end{noteBox}}
\newcommand{\warning}[1]{\begin{hintBox} \textbf{Warning:} #1 \end{hintBox}}
\newcommand{\important}[1]{\begin{importantBox} \textbf{Important:} #1 \end{importantBox}}

\hypersetup{citecolor=blue}

\definecolor{MSBlue}{rgb}{.204,.353,.541}
\usepackage{titlesec}
\titleformat*{\section}{\color{MSBlue}}
\titleformat*{\subsection}{\color{MSBlue}}
\setlength{\parindent}{0pt}


% Title and author information
\title{Residual and ensemble regression for improved reconstruction of high-energy gamma-ray events}
\author{Gernot Maier}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Ground-based imaging atmospheric Cherenkov telescopes (IACTs) detect the faint
Cherenkov light
emitted by air showers initiated by high-energy gamma rays.
IACTs are the primary instruments to detect these photons above tens of GeV.
Multiple analysis methods exist to reconstruct direction and energy,
with performance and robustness varying between depending on observation conditions
and gamma-ray energy.
This paper presents a machine-learning approach based on gradient-boosted regression trees
that learns from the errors of existing methods and combine their results.
Evaluation based on simulated events and observations of the Crab Nebula shows
significant improvements in angular and energy resolution over existing methods.
\end{abstract}

\section{Introduction}

Gamma-ray astronomy uses Imaging Atmospheric Cherenkov Telescopes (IACTs)
to measure the energy and direction of photons from
tens of GeV to hundreds of TeV (e.g., \citep{2009ARA&A..47..523H}).
The telescopes record Cherenkov light from air showers using
large reflectors, pixelated cameras combined with fast electronics and trigger systems.
Modern observatories measure projections of the longitudinal development of air showers
with multiple telescopes simultaneously,
enabling stereoscopic reconstruction of primary direction and energy.
The accuracy of this technique is limited by intrinsic air-shower fluctuations and
telescope photon-detection efficiency to estimated
1 (0.3) arcmin angular resolution at 100 GeV (1 TeV)
\citep{hofmann2006performancelimitscherenkovinstruments}.
Systematic uncertainties from varying observation conditions (e.g., night-sky background light,
atmospheric conditions) further limit the achievable performance and require robust
reconstruction methods.

The fields knows a variety of reconstruction methods for direction and energy estimation,
including robust and efficient geometric methods based on image parameters
\citep{1985ICRC....3..445H, 1997APh.....8....1D, 1999APh....12..135H},
template-model based methods \citep{2001A&A...374..895P,2006APh....25..195L, 2009APh....32..231D,2014APh....56...26P},
and machine-learning-based methods using
Hillas parameters \citep{2024A&A...691A.328A} or full camera images.
Performance varies across methods, with template-model based algorithms achieving
the highest precision at the cost of computational complexity and robustness.

This paper shows how a simple machine-learning-based regression layer
can improve the performance of existing image-parameter based
reconstruction methods.
The method adds a thin layer to existing analysis pipelines and obtains performance gains
while retaining the robustness of the established methods.
The explicit goal is to use this method for archival data from almost two
decades of observations of the VERITAS IACT array \citep{2006APh....25..391H},
without a complete re-write of the analysis code or low-level reprocessing
of raw data.
The method is robust enough to be easily adapted to different IACT arrays,
as demonstrated here for both VERITAS and the Northern array of the Cherenkov Telescope Array Observatory (CTAO)
\citep{2019scta.book.....C}.

The paper is structured as follows:
Section 2 describes the data pipeline and feature engineering.
Section 3 discusses the regression targets and performance metrics.
Section 4 presents the model architecture and training strategy.
Section 5 evaluates the performance and Section 7 discusses systematic effects and robustness.
Section 7 concludes with summary and outlook.

The code developed for this paper is available from \citep{maier_2026_18412988}.
All methods and results presented here are based on the open-source Eventdisplay reconstruction
code \citep{maier_2025_17447664,maier_2026_18412887}.

\section{Data Pipeline and Feature Engineering}


\begin{itemize}
  \item IACT event reconstruction
  \item Energy and direction regression
  \item Machine-learning-based reconstruction
    \item Limitations of classical geometric reconstruction (Hillas-based intersection).
    \item Objective: Improving angular and energy resolution via Multi-target Regression.
    \item Motivation for Gradient Boosting in high-dynamic-range scientific data.
  \item Standard Eventdisplay methods
  \item Statement of the Problem
  \item Residual learner - learn from the errors made by the existing reconstruction methods
e.g. improve reconstruction of truncated images
  \item Ensemble learner - learn how to combine the results from the different reconstruction methods
energy is reconstructed with look-up tables and DispBDTs
\item direction is reconstructed by the intersection of lines method and DispBDT (disp provides an average plus results per telescopes)
use image parameter / additional features for support
\item use >120 training features (including new ones like the angle towards the geomagnetic field)
\end{itemize}

\begin{itemize}
  \item Training with Monte Carlo air-shower simulations
  \item Array layout, telescope multiplicity
  \item Energy, zenith, offset distributions
  \item Eventdisplay-derived parameters
  \item sorting by mirror area (telescope type) and size
    \item \textbf{Ground Truth Labels:} Monte Carlo simulated primary energy ($MCe0$) and arrival direction offsets ($MCxoff, MCyoff$).
    \item \textbf{Stereo Input Features:} Multi-telescope image parameters (Size, Width, Length, Distance, $\chi^2$ emission height).
    \item \textbf{Geomagnetic Corrections:} Calculation of shower angles relative to the Earth's magnetic field ($\theta_B$) to account for Lorentz force broadening.
    \item \textbf{Data Transformation:} Logarithmic scaling of intensity (Size) and energy to handle power-law distributions.
    \item \textbf{Handling Multiplicity:} Flattening heterogeneous telescope array data into unified feature vectors for $N$-telescope events.
  \item clipping and transformations (log size, log energy)
  \item Train / validation / test samples
  \item Evaluation with Crab Nebula data
\end{itemize}

\section{Regression Targets and Performance Metrics}

\begin{itemize}
  \item Energy bias
  \item Energy resolution (68\% containment)
  \item Angular resolution (PSF containment)
\end{itemize}

\section{Model Architecture and Training}

\begin{itemize}
  \item regression targets: energy, direction (x,y)
  \item multi-target regression
  \item gradient-boosted decision trees (XGBoost)
  \item Gradient-boosted decision trees
  \item Loss functions
  \item Regularization and depth control
    \item \textbf{Hyperparameters:} Optimization of tree depth (max\_depth=10), learning rate (0.1), and subsampling (0.7).
    \item \textbf{Loss Function:} \texttt{reg:squarederror} optimized for residuals in spatial and energy domains.
    \item \textbf{Weighted Learning:} Implementation of inverse-count energy weighting and multiplicity weighting (proportional to $N_{images}^2$) to balance the training set.
\end{itemize}

\textbf{Suggested tables:}
\begin{itemize}
  \item Hyperparameter summary
\end{itemize}

\subsection{Training and Validation Strategy}

\begin{itemize}
    \item feature importance
    \item feature correlations
  \item Independent test sample
  \item Energy-binned evaluation
  \item Overfitting control
\end{itemize}

\textbf{Suggested plots/tables:}
\begin{itemize}
  \item Feature correlation matrix
  \item Feature importance ranking
  \item Training vs validation loss
\end{itemize}

\section{Performance Evaluation}

\textbf{Suggested plots/tables:}
\begin{itemize}
  \item \textbf{Interpretability:} Global feature importance ranking (SHAP/Gain) for energy vs. directionality.
  \item Energy resolution vs true energy
  \item Angular resolution vs energy
  \item Energy bias vs true energy
  \item cumulative distributions for MC and Crab data
  \item theta2 plots for Crab data
\end{itemize}

\section{Systematic Effects and Robustness}
\begin{itemize}
  \item Zenith-angle dependence
  \item Offset dependence
  \item Domain shift sensitivity
  \item missing telescopes (3-telescope reconstruction)
\end{itemize}
\textbf{Suggested plots:}
\begin{itemize}
  \item Performance vs zenith
  \item Performance vs offset
\end{itemize}

\section{Conclusions}
\begin{itemize}
  \item Impact
  \item Limitations
  \item Summary of regression performance
\end{itemize}


\section*{AI usage disclosure}

Generative AI tools (including Claude, ChatGPT, and Gemini) were used to assist
with code development and debugging of the code associated to this paper.
AI tools are also used for language improvement of this text.
All AI-assisted outputs were reviewed, validated, and, where necessary,
modified by the authors to ensure accuracy and reliability.

\section*{Acknowledgments}

Add acknowledgments here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{aasjournal}
\bibliography{bibliography}%

\end{document}
